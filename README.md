# Ollama Local Chat

A desktop chat application built with Electron.js that uses local Llama models for AI responses, featuring conversation history and a ChatGPT-like interface.

## Features

- Real-time streaming responses
- Code formatting and syntax highlighting
- Conversation history
- Thinking mode animation
- System prompt customization
- Model parameter adjustments
- Multiple model support

## Prerequisites

- Node.js (v14 or later)
- Ollama installed and running
- A compatible Llama model downloaded through Ollama

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/ollama-local-chat.git
cd ollama-local-chat
```

2. Install dependencies:
```bash
npm install
```

3. Start the application:
```bash
npm start
```

## Configuration

You can customize various settings in the `config.js` file:
- Default model
- System prompt
- API endpoint
- Model parameters

## Contributing

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Ollama team for the local LLM runtime
- Electron.js community
- All contributors